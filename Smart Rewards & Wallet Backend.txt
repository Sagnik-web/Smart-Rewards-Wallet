Smart Rewards & Wallet Backend


1. Why did you choose your folder structure?
	
I structured the backend using a Controller-Service-Model pattern to keep the concerns strictly separated. I use Controllers as the entry point to handle HTTP requests and responses, while the core business logic and database transactions live in the Service layer. This makes the logic much easier to unit test since it's decoupled from the web server. For data structure, I use Mongoose Models, and I've offloaded cross-cutting tasks like authentication and error handling to dedicated Middlewares. It’s a clean, industry-standard layout that scales well as the app grows.




2. How did you implement wallet atomicity to avoid race conditions?

Financial operations require strict atomicity, so I used Mongoose transactions to ensure that wallet balances and transaction histories never get out of sync. By combining transactions with conditional $inc operations, I eliminated the risk of 'double-spending' or race conditions. If a user tries to redeem a voucher, the update only triggers if the balance requirement is met mid-transaction—otherwise, the system safely rolls back. This makes the wallet logic both resilient and scalable





3. Explain the trade-offs in your indexing strategy

My indexing strategy is focused on optimizing the most frequent user paths. I use a unique index on email for O(1) login lookups and have indexed userId across the transactions, rewards, and wallet collections. Since our most common queries are user-centric—like fetching a transaction history or balance—this drastically reduces collection scans. I’ve intentionally held off on compound indexes to avoid unnecessary write overhead and storage bloat; I prefer to let profiling tools like explain() dictate when a specific pattern, like userId plus a timestamp range, actually requires a combined index.





4. How would you scale this system if user count reaches 10 million?

To scale this to millions of users, I’d shift from a monolithic setup to a distributed architecture. On the data layer, I’d move to a sharded MongoDB cluster using a hashed userId as the shard key to ensure even data distribution. To handle high write throughput, I’d implement a CQRS-inspired pattern, where writes land in an append-only transaction log and reads are served via materialized views or a Redis cache layer. This keeps the primary database from becoming a bottleneck. Computationally, I’d move heavy tasks like reporting to independent worker clusters and use an API gateway to manage traffic across autoscaling, stateless Express instances.



5. What do you consider the riskiest part of your current design?

The biggest challenge with a system like this is handling concurrent balance updates. In a live environment, you run the risk of race conditions where two requests hit the same wallet at the exact same time. To prevent 'double-spending,' I rely on MongoDB transactions to make sure that the balance update and the transaction log are treated as a single, atomic unit. I back this up by running high-concurrency load tests to prove the logic holds up even when the server is being hammered.
I’m also very intentional about indexing. It’s easy to over-index for speed, but that actually hurts you by causing 'write amplification,' making every transaction heavier for the database. My approach is to profile the queries first and only add indexes that are strictly necessary to keep things lean.
Finally, for security, I focus on protecting the 'keys to the kingdom.' This means moving beyond just basic auth to using secure environment storage, rotating tokens, and implementing strict API throttling. It’s all about making sure a single leaked secret or a bot attack can’t bring the whole system down.

